<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="LongVideoAgent: Multi-Agent Reasoning with Long Videos">
  <meta name="keywords" content="Long Video, Multi-Agent, Reasoning, Video Understanding">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LongVideoAgent: Multi-Agent Reasoning with Long Videos</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">LongVideoAgent: Multi-Agent Reasoning with Long Videos</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Runtao Liu<span style="font-size: smaller;">*</span>,</span>
            <span class="author-block">Ziyi Liu<span style="font-size: smaller;">*</span>,</span>
            <span class="author-block">Jiaqi Tang,</span>
            <span class="author-block">Yue Ma,</span>
            <span class="author-block">Renjie Pi,</span>
            <span class="author-block">Jipeng Zhang,</span>
            <span class="author-block">Qifeng Chen</span>
        </div>
        <div class="is-size-5 publication-authors">
          <span class="author-block">Hong Kong University of Science and Technology</span>
        </div>
        <div class="is-size-6" style="margin-top: 10px;">
            <span>{rliuay@connect.ust.hk, ziyiliu0811@outlook.com}</span>
        </div>
        <div class="is-size-6" style="margin-top: 10px;">
            <span>* Equal Contribution. Work done during Ziyi's internship in HKUST.</span>
        </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- arXiv Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2512.20618" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- HF Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/papers/2512.20618" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <p style="font-size:18px">ðŸ¤—</p>
                  </span>
                  <span>Hugging Face</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/longvideoagent/LongVideoAgent" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advances in multimodal LLMs and systems that <i>use tools</i> for long-video QA point to the promise of reasoning over hour-long episodes. 
            However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. 
            We propose <b>LongVideoAgent</b>, a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. 
            The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. 
            This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. 
            On our proposed <i>LongTVQA</i> and <i>LongTVQA+</i> which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. 
            Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            Traditional single-pass MLLMs that ingest entire long videos in one contextâ€”typically (may through heavy downsampling and compression) often miss crucial evidence and produce wrong answers, whereas LongVideoAgent conducts multi-agent, multi-round, and multimodal reasoning to extract sparse, task-relevant cues and answer correctly.
          </p>
          <div class="has-text-centered">
            <img src="https://github.com/longvideoagent/LongVideoAgent/raw/main/readme_src/teaser.png" alt="Overview Teaser" style="width: 100%; max-width: 600px;">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method: Multi-Agent Framework</h2>
        <div class="content has-text-justified">
          <div class="has-text-centered">
            <img src="https://github.com/longvideoagent/LongVideoAgent/raw/main/readme_src/architecture.png" alt="Architecture" style="width: 100%; max-width: 800px;">
            <p>
              Architecture of LongVideoAgent. A MasterAgent runs for up to \(K\) rounds, collaborating with a GroundingAgent to localize relevant clips from videos and a VisionAgent to read fine-grained cues from the localized frames. Evidence accumulates until the MasterAgent feels confident to answer the user.
            </p>
          </div>
          <h3 class="title is-4">Iterative Reasoning Loop</h3>
          <p>
            Unlike single-pass models, LongVideoAgent operates in a bounded loop (max \(K\) steps). At each step, the MasterAgent generates a "thinking" trace and emits a structured action token:
          </p>
          <ul>
            <li><code>&lt;request_grounding&gt;</code>: Calls the GroundingAgent to localize relevant video segments based on subtitles. The agent returns a symbolic tag <code>&lt;clip_X&gt;</code>.</li>
            <li><code>&lt;visual_query&gt;</code>: Calls the VisionAgent to extract specific visual details (objects, actions, text) from the localized clip. The agent returns textual observations.</li>
            <li><code>&lt;answer&gt;</code>: Terminates the loop and provides the final response when sufficient evidence is gathered.</li>
          </ul>
          <h3 class="title is-4">Reinforcement Learning (GRPO)</h3>
          <p>
            We optimize the MasterAgent using Group Relative Policy Optimization (GRPO). The training objective includes: 1. Structural validity. 2. Answer Correctness: Rewarding the agent for reaching the correct final answer.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experimental Results</h2>
        <div class="content has-text-justified">
          <p>
            We evaluate LongVideoAgent on LongTVQA and LongTVQA+, which are episode-level datasets.
          </p>
          <h3 class="title is-4">Main Results</h3>
          <div class="has-text-centered">
            <img src="https://github.com/longvideoagent/LongVideoAgent/raw/main/readme_src/main_result.png" alt="Main Results" style="width: 100%; max-width: 800px;">
          </div>
          <p>
            Performance on LongTVQA and LongTVQA+. The left block lists model attributes (Agentic, Input, RL fine-tune); the right block reports validation accuracy (%). GPT-4o and Gemini-2.5 Pro are multimodal baselines that process and accept the full long video directly. Methods labeled <code>Agentic</code> indicate the model operates as the MasterAgent; methods labeled <code>AgenticRL</code> additionally denote RL fine-tuning. Parenthesized green numbers denote absolute gains over the immediately preceding (non-agentic or non-RL) setting. We observe that: (i) our multi-agent framework, LongVideoAgent, consistently outperforms the non-agentic counterparts; (ii) agentic RL yields additional gains, especially for smaller open-source models; (iii) using frames provides visual evidence beyond subtitles, and generally outperforms subtitle-only inputs; (iv) closed-source models remain strong, but the gap narrows much when open-source models adopt agentic designs and agentic RL.
          </p>
          <h3 class="title is-4">Ablation Analysis</h3>
          <div class="has-text-centered">
            <img src="https://github.com/longvideoagent/LongVideoAgent/raw/main/readme_src/ablation.png" alt="Ablation Analysis" style="width: 100%; max-width: 600px;">
          </div>
          <p>
            We conduct comprehensive ablation studies to validate our design choices. First, we observe that both grounding and vision agents are essential, with the full multi-agent system achieving the highest accuracy. Second, increasing the reasoning step limit \(K\) improves performance until saturation, confirming the value of iterative planning. Finally, stronger vision backbones and larger temporal windows provide richer context, further boosting the agent's reasoning capabilities.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{liu2025longvideoagentmultiagentreasoninglong,
      title={LongVideoAgent: Multi-Agent Reasoning with Long Videos}, 
      author={Runtao Liu and Ziyi Liu and Jiaqi Tang and Yue Ma and Renjie Pi and Jipeng Zhang and Qifeng Chen},
      year={2025},
      eprint={2512.20618},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2512.20618}, 
}</code></pre>
  </div>
</section>


</body>
</html>
